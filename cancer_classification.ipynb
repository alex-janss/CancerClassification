{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cancer_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If2DRWB3orpr",
        "colab_type": "text"
      },
      "source": [
        "# Cancer Classification using Microarray Data\n",
        "Alex Janss\n",
        "\n",
        "## Introduction\n",
        "The GEO database is home to thousands of microarray datasets. These datasets measure the relative gene expression across thousands of selected genes for a given patient sample. Each value in the output roughly corresponds to the abundance of an RNA molecule of a given type (corresponding to the given gene) for a particular sample, with higher values indicating more RNA molecules. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebh2YKZqrx-8",
        "colab_type": "text"
      },
      "source": [
        "Microarrays are an extremely important source of data, as they are very cheap and easy to run and yeild lots of information about the state of the cells from the sample. The gene expression data obtained form the cells yeilds information about the cells' activity and can be used to distinguish types of cells (eg. brain vs liver) and states of cells (eg. healthy vs diseased).\n",
        "\n",
        "The dataset used in this experiment has 174 different samples, each from one of nine different types of cancer (Breast, Central Nervous System, Colon, Leukemia, Melanoma, Non-Small Cell Lung, Ovarian, Prostate, and Renal). My goal is to create a model that will be able to sucessfuly classify a sample into one of the 9 cancer type by the microarray expression data. I will be using **Naive Bayes, Random Forest, and Gradient Boosting** to classify the data.\n",
        "\n",
        "Importing the necessary libraries and data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6iSN2fzO2F0",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "ba080aa5-3795-40e1-ca9c-11ae15490a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "path = ''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnTrIHqSSYFu",
        "colab_type": "code",
        "outputId": "df6f04e2-4f4d-4aba-a3c0-672d3d72f7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28TEsGUxWCD-",
        "colab_type": "text"
      },
      "source": [
        "## Data Description\n",
        "Lets load the data and take a look at the first few rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F5WE8W4PMvI",
        "colab_type": "code",
        "outputId": "4cb11563-c4ab-461f-aa74-bf966e656bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# load data\n",
        "try:\n",
        "    labels = pickle.load(open(path + \"labels.pickle\", \"rb\"))\n",
        "    data = pickle.load(open(path + \"data.pickle\", \"rb\"))\n",
        "except (OSError, IOError) as e:\n",
        "    df = pd.read_csv(path + 'GDS4296_table.csv', low_memory=False, index_col=0)\n",
        "    df = df.T\n",
        "    df.drop(columns='#NAME?', inplace=True)  # drop columns w/ missing names\n",
        "    df.sort_values(by='Cell_type')   # sort by label\n",
        "    data = df.iloc[:,1:]  # separate data and labels\n",
        "    labels = df.iloc[:,0]\n",
        "    data=(data-data.mean())/data.std()  # rescale (normalize) the variables\n",
        "    pickle.dump(data, open(path+\"data.pickle\", \"wb\"))\n",
        "    pickle.dump(labels, open(path+\"labels.pickle\", \"wb\"))\n",
        "\n",
        "print(data.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            MIR4640      RFC2     HSPA6  ...     STAT1     STAT1     STAT1\n",
            "GSM803615 -1.195915  1.013012  2.089454  ... -0.651119 -0.545434 -0.698218\n",
            "GSM803674 -2.553449  1.079588  2.082869  ... -0.607116 -0.653362 -0.797997\n",
            "GSM803733 -1.885320  0.825075 -0.575355  ... -1.219383 -1.341147 -1.380196\n",
            "GSM803616 -2.351636  0.685868 -0.109988  ... -0.738815 -0.520803 -0.533830\n",
            "GSM803675 -1.999497  0.380964  0.005226  ... -0.578612 -0.624779 -0.836517\n",
            "\n",
            "[5 rows x 54623 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcdWOeT6WdH1",
        "colab_type": "text"
      },
      "source": [
        "Each row (\"GSM...\") represents a single patient sample that we will try to classify into one of the 9 types of cancers. There are 174 samples.\n",
        "\n",
        "The `labels` variable designates which type of cancer each sample came from.\n",
        "\n",
        "The remaining columns labeled by gene name represent the level of a particular RNA molecule within the given sample*. There are 54,676 genes. Note that I have rescaled (normalized) the genes accross the 174 samples.\n",
        "\n",
        "\n",
        "To state the obvious, this is an extremely high dimensionality dataset, with 54,676 predictors and only 174 data points. This will be a major challenge in fitting models to this data.\n",
        "\n",
        "Lets take a look at the distributions of a few of the variables:\n",
        "\n",
        "\n",
        "*Note: The value represents the relative expression of that RNA and cannot be used to determine the absolue quantity of an RNA molecule, and cannot be compared accross columns. Thus not much information is lost in rescaling.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8151c7e6-df6c-4d36-8093-74a3dbf5679a",
        "id": "mhjq74-4XaeY",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# histograms\n",
        "sns.set()\n",
        "gene1, gene2, gene3 = 100, 200, 300\n",
        "fig, axes = plt.subplots(ncols=3, sharex=True, sharey=True, figsize=(8,4))\n",
        "data.iloc[:,gene1].plot.hist(ax=axes[0])\n",
        "data.iloc[:,gene2].plot.hist(ax=axes[1])\n",
        "data.iloc[:,gene3].plot.hist(ax=axes[2])\n",
        "axes[0].set_xlabel('Level of gene ' + df.iloc[:,gene1].name)\n",
        "axes[1].set_xlabel('Level of gene ' + df.iloc[:,gene2].name)\n",
        "axes[2].set_xlabel('Level of gene ' + df.iloc[:,gene3].name)\n",
        "axes[0].set_ylabel('# of samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '# of samples')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEMCAYAAADd1S/AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnBBgZP-i94W",
        "colab_type": "text"
      },
      "source": [
        "The distribution of the samples appears to be (somewhat) normal for these few genes. Now that we have some idea of what the data looks like, we will begin classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME1dLQzOwyE5",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian Naive Bayes\n",
        "Naive Bayes is arguably one of the simplest classification techniques in machine learning. There are no hyperparameters to tune, one simply needs to run the model. Here I use the Gaussian version of the algorithm, as the predictors are continuous not categorical. \n",
        "\n",
        "Let's test this method via 5-fold cross validation. I use the stratified version of train-test split, as not all the classes are equally represented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwRsf6hSF2My",
        "colab_type": "code",
        "outputId": "dce2b9a1-a651-414c-c0ee-af50725dd02c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GNB, 5-fold cross validation.\n",
        "random.seed(711)\n",
        "gnb_mod = GaussianNB()   # Create a Gaussian Classifier\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "error = []\n",
        "for train_index, test_index in skf.split(data, labels):\n",
        "  X_train, X_test = data.iloc[train_index], data.iloc[test_index]  # train-test split\n",
        "  y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "  gnb_mod.fit(X_train, y_train)    # fit on train\n",
        "  y_pred = gnb_mod.predict(X_test)  # predict on test\n",
        "  error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "print(sum(error)/len(error))  # average accuracy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9680357142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60MBwqo_edbV",
        "colab_type": "text"
      },
      "source": [
        "96.8% prediction accuracy! This is impressive for a model that took no work to tune and a difficult data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b6-sRysWNOx",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest\n",
        "Random forest is an extremely popular out-of-the-box ML tool that works very well with very little tuning on a variety of datasets. It works by averaging a large number of high-variance decision trees. Here we will test RF with 500, 1000, and 2000 trees using 5-fold CV. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZO-2wfMWhPJ",
        "colab_type": "code",
        "outputId": "474a3c42-13e9-4afd-8c8e-0a44992163cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# RF, 5-fold CV\n",
        "random.seed(731)\n",
        "num_trees = [500,1000,2000]  # hyperparameters to test\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "error = []\n",
        "for param in num_trees:\n",
        "  rf_mod = RandomForestClassifier(n_estimators=param)   # create RF model\n",
        "  for train_index, test_index in skf.split(data, labels):\n",
        "    X_train, X_test = data.iloc[train_index], data.iloc[test_index]  # train-test split\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    rf_mod.fit(X_train, y_train)    # fit on train\n",
        "    y_pred = rf_mod.predict(X_test)  # predict on test\n",
        "    error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(\"Accuracy:\",sum(error)/len(error), \"with\", param, \"trees\")  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9766071428571429 with 500 trees\n",
            "Accuracy: 0.9825892857142857 with 1000 trees\n",
            "Accuracy: 0.9883928571428572 with 2000 trees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpA1Q43siBo-",
        "colab_type": "text"
      },
      "source": [
        "Our best model performed with 98.8% accuracy. This is an incredible performance on a model that also took very little tuning. RF models are often used a baseline for testing the performance of more complex models. 98.8% is a very high bar to beat! I will now attempt to beat this with other models, but if not, this accuracy is stellar performance given the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWoIwqEJVDNx",
        "colab_type": "text"
      },
      "source": [
        "## Boosting\n",
        "Tree boosting is a powerful method that combines many weak learners (small trees) succesively to create an aggregate model. Each model takes much longer to run and tune than the methods used above, as the model building process is computationally expensive and there are more hyperparameters. As an illustration I run a single boosting model with 5-fold CV using sci-kit learn's implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9T2iQCNVQAt",
        "colab_type": "code",
        "outputId": "7cd52fa8-e294-40fe-9d3a-9381a418251a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Boosting, 5-fold CV\n",
        "random.seed(1699)\n",
        "boost_mod = GradientBoostingClassifier(n_estimators=100,learning_rate=.1,max_depth=2)   # create boosting model\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "error = []\n",
        "for train_index, test_index in skf.split(data, labels):\n",
        "  X_train, X_test = data.iloc[train_index], data.iloc[test_index]  # train-test split\n",
        "  y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "  boost_mod.fit(X_train, y_train)    # fit on train\n",
        "  y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "  error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "print(sum(error)/len(error))  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8578571428571429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqaBXsKXlACe",
        "colab_type": "text"
      },
      "source": [
        "As is evident, this model needs much more tuning in order to be able to perform well. I will now perform a grid search using a single train-test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni5m5hep0hxn",
        "colab_type": "code",
        "outputId": "d90ba8ea-dc59-492c-efa6-49ae7d6e94ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "# Boosting, Grid search\n",
        "random.seed(711)\n",
        "n_trees = np.array([50,100,500])  \n",
        "rate = np.array([.001, .01, .1])\n",
        "depth = np.array([2,4])\n",
        "params = np.array(np.meshgrid(n_trees, rate, depth)).reshape(3,-1).T  # grid search parameters\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify=labels, test_size=0.2)  # single train-test split\n",
        "error = []\n",
        "\n",
        "# create boosting model for each combination of hyperparameters\n",
        "for p in params:\n",
        "  boost_mod = GradientBoostingClassifier(n_estimators=int(p[0]),learning_rate=p[1],max_depth=int(p[2]))\n",
        "  boost_mod.fit(X_train, y_train)\n",
        "  y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "  error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(p, error[-1])\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.e+01 1.e-03 2.e+00] 0.6857142857142857\n",
            "[5.e+01 1.e-03 4.e+00] 0.6\n",
            "[1.e+02 1.e-03 2.e+00] 0.9428571428571428\n",
            "[1.e+02 1.e-03 4.e+00] 0.9428571428571428\n",
            "[5.e+02 1.e-03 2.e+00] 0.9714285714285714\n",
            "[5.e+02 1.e-03 4.e+00] 0.9714285714285714\n",
            "[5.e+01 1.e-02 2.e+00] 0.9714285714285714\n",
            "[5.e+01 1.e-02 4.e+00] 0.9714285714285714\n",
            "[1.e+02 1.e-02 2.e+00] 0.9714285714285714\n",
            "[1.e+02 1.e-02 4.e+00] 0.9714285714285714\n",
            "[5.e+02 1.e-02 2.e+00] 0.9714285714285714\n",
            "[5.e+02 1.e-02 4.e+00] 0.9714285714285714\n",
            "[50.   0.1  2. ] 0.9714285714285714\n",
            "[50.   0.1  4. ] 0.9714285714285714\n",
            "[100.    0.1   2. ] 0.9714285714285714\n",
            "[100.    0.1   4. ] 0.9714285714285714\n",
            "[5.e+02 1.e-01 2.e+00] 0.9714285714285714\n",
            "[5.e+02 1.e-01 4.e+00] 0.9714285714285714\n",
            "[0.6857142857142857, 0.6, 0.9428571428571428, 0.9428571428571428, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714, 0.9714285714285714]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1keJO2vPleL1",
        "colab_type": "text"
      },
      "source": [
        "This grid search ended up being unhelpful. Because of the single train-test split, most of the parameter combinations yeilded the same accuracy scores. Unfortunately, we will need to use 5-fold CV to more precicely evaluate the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey_6JXw4gfxD",
        "colab_type": "code",
        "outputId": "91f81e78-b73f-4c30-8d3c-901b3a3d656f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        "# Boosting, 5-fold CV, Grid search\n",
        "random.seed(711)\n",
        "n_trees = np.array([50,100,500])  \n",
        "rate = np.array([.001, .01, .1])\n",
        "depth = np.array([2,4])\n",
        "params = np.array(np.meshgrid(n_trees, rate, depth)).reshape(3,-1).T  # grid search params\n",
        "\n",
        "# 5-fold CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "for p in params:\n",
        "  boost_mod = GradientBoostingClassifier(n_estimators=int(p[0]), learning_rate=p[1], max_depth=int(p[2]))  # create boosting model\n",
        "  error = []\n",
        "  for train_index, test_index in skf.split(data, labels):\n",
        "    X_train, X_test = data.iloc[train_index], data.iloc[test_index]  # train-test split\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    boost_mod.fit(X_train, y_train)    # fit on train\n",
        "    y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "    error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(sum(error)/len(error), p)  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7244642857142857 [5.e+01 1.e-03 2.e+00]\n",
            "0.7485714285714286 [5.e+01 1.e-03 4.e+00]\n",
            "0.8132142857142857 [1.e+02 1.e-03 2.e+00]\n",
            "0.8324999999999999 [1.e+02 1.e-03 4.e+00]\n",
            "0.9244642857142857 [5.e+02 1.e-03 2.e+00]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-90edd4611e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# train-test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mboost_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# fit on train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboost_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# predict on test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# model accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1545\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1608\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m   1609\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1244\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvtYPV6umd_M",
        "colab_type": "text"
      },
      "source": [
        "Here the run has been manually terminated after just 5 sets of hyperparameters have been tested. Even this took over 4 hours to run on my machine. This problem is quickly becoming intractable; Even with this very course grid, fitting 18 x 5 = 90 boosting models on 50,000 variables would take too long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkVv7DEcOIDv",
        "colab_type": "text"
      },
      "source": [
        "## Variable Selection\n",
        "The time being taken is due to the 50,000+ variables that the model is trying to evaluate. In order to reduce the dimensionality and make this problem more tractable, we need to remove some of the variables.\n",
        "\n",
        "The sklearn random forest implementation comes with a `feature_importances` attribute. It is determined by assessing the mean decrease in node impurity across the 100s of trees: variables that split the classes with greater purity are assigned a higher importance. We will use these importance scores to select which variables to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF8cHO_82OAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RF variable importance\n",
        "rf_mod = RandomForestClassifier(n_estimators=1000, random_state=3314)   # create RF model\n",
        "_ = rf_mod.fit(data, labels)    # fit on entire dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOeZY-rn4Jhw",
        "colab_type": "code",
        "outputId": "9ff648a5-8e3e-4788-cd29-d56f71b39833",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "#@title \n",
        "var_import=np.sort(var_import)[::-1]\n",
        "total, result = 0.0, 0\n",
        "for i in range(len(var_import)):\n",
        "  total = total + var_import[i]\n",
        "  if total > .90:\n",
        "    result = i\n",
        "    break\n",
        "\n",
        "plt.plot(np.arange(54623), var_import)\n",
        "plt.fill_between(np.arange(54623),var_import)\n",
        "line1= plt.axvline(x=result,color='gray')\n",
        "plt.legend((line1,), (\"90th percentile\",))\n",
        "plt.title(\"Variable Importances\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Variable Importances')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de5xXZd3v/9d7hpOhgcJkKCiYaA6l\npNykZbfskMRS8d633uFdicXeZmm79n1IzHtTNz/Zaf12pqn50/DnIRO4KXW2m/KQlXbwgHmIg+SA\nqBAijoiCcpjhs/9Y1+CaL9/vzJrh8B2m9/Px+D5mrWtd61rXNXz5fua6rvVdlyICMzOzImqqXQEz\nM9t7OGiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGtZjSDpE0gZJtQXyjpO0sp3jN0u6bNfW\n0Gzv56BhVSHpF5JmlEmfJOllSb06W2ZEvBgR+0ZEy66pZddICkmHV7MOrSStkHRytethPYeDhlXL\nLcBnJakk/XPA7RHR3JnCuhJkejL/Pmx3cdCwarkLGAR8rDVB0v7AacCtaf9Tkp6U9IaklyR9K5d3\nePqLfqqkF4EHc2m9Up7PS1oi6U1JyyV9sbQSkr4h6dX0F/lnKlVW0mmSnpL0uqTfSzq6SCMlfUvS\nf0j6carHnyQdIekSSa+kdn0il//Xkr4t6bHU7rslHZA7foakRakev5Z0VO7YCkkXS3oG2CjpDuAQ\n4H+nYbuvp3z/kXpz6yU9JGlUroybJV0r6f+k+j4q6X2546Mk3S/pNUlrJH0jpddImiZpmaQmSXNb\n6y2pX2p/U6r345IOLPL7s+7HQcOqIiLeBuYC5+aS/wF4NiKeTvsb0/GBwKeAL0k6s6Sok4CjgFPK\nXOYVsiD0buDzwJWSjs0dfy8wGDgYmALcIOnI0kIkfQi4CfgiWaD7/4AGSX0LNvd04DZgf+BJ4F6y\n/3sHAzNSeXnnAl8AhgDNwNWpHkcAdwBfA+qA+WQBoU/u3HPIflcDI+Ic4EXg9DRs952U5+fASOA9\nwB+B20uuPxn491TfRmBmuv5+wAPAL4CDgMOBX6ZzvgKcSfbvcRCwDrg2HZsCDACGkf3+LgDe7uiX\nZt1URPjlV1VewInA60C/tP874L+3k//7wJVpezgQwGG5461pvSqcfxfw1bQ9juwDuX/u+Fzgf6Tt\nm4HL0vYPgf+npKylwEkVrhPA4Wn7W8D9uWOnAxuA2rS/X8o/MO3/Grg8l78e2ALUAv8DmJs7VgOs\nAsal/RXAF0rqsgI4uZ3f6cB0/QG5dv8od/yTZIEcsoD0ZIVylgDjc/tDgK1AL7IA+Hvg6Gq/5/za\n+Zd7GlY1EfFb4FXgzDQEMhb4SetxSR+W9CtJayWtJ/sLdXBJMS9VKl/SqZIeSUMpr5N9AObPXxcR\nG3P7L5D9lVzqUOCf09DK66msYRXylrMmt/028Gq8M1nf+hf3vrk8+Ta9APRO9T4o7QMQEdtS3oMr\nnLsDSbWSLk/DSG+QBRVo+3t5Obf9Vq5uw4BlFYo+FLgz9/tZArQAB5L1su4FZkv6i6TvSOrdXj2t\n+3LQsGq7lWw45rPAvRGR/4D9CdAADIuIAcD1QOnEednHNKeho58C/y9wYEQMJBvOyZ+/v6T+uf1D\ngL+UKe4lYGZEDMy93hURdxRuZecMK6nTVrLg+heyD2cA0k0Ew8h6G61Kfx+l+/8ITAJOJhsyGt5a\nXIF6vQQc1s6xU0t+R/0iYlVEbI2If4+IeuAjZEOG51Yox7o5Bw2rtlvJPsD+K9kdVXn7Aa9FxCZJ\nY8k+8IrqA/QF1gLNkk4FPlEm379L6iPpY2QfZv9RJs+NwAWp5yNJ/dMk/X6dqE9nfFZSvaR3kc15\nzEs9k7nApySNT3+p/zOwmWzop5I1tP2g3y+d0wS8C/ifnajXPcAQSV+T1FfSfpI+nI5dD8yUdCiA\npDpJk9L2f5L0QWXfn3mDLAhu68R1rRtx0LCqiogVZB96/cl6FXlfBmZIehOYTvahWbTcN4H/ls5Z\nRxZwSst/OR37C9lk8AUR8WyZshaQBbVrUv5G4LyidemC28jmFl4G+pG1g4hYStYj+wFZz+N0sknu\nLe2U9W3g39Kw0b+QBekXyHoni4FHilYq/U4npOu+DDwH/Kd0+Cqy3+996d/rEaA1oLwXmEcWMJYA\nv0lttL2QIrwIk1l3IenXwI8j4kfVrotZOe5pmJlZYQ4aZmZWmIenzMysMPc0zMyssB79ULPBgwfH\n8OHDq12NXa6pqQmAQYMGVbkmZtYTPfHEE69GRF25Yz06aAwfPpwFCxZUuxq73M033wzAeeedV9V6\nmFnPJOmFSsc8PGVmZoU5aJiZWWEOGmZmVliPntMws91v69atrFy5kk2bNlW7KtZJ/fr1Y+jQofTu\nXfyhww4aZrZTVq5cyX777cfw4cPRDqv3WncVETQ1NbFy5UpGjBhR+DwPT5nZTtm0aRODBg1ywNjL\nSGLQoEGd7iEWChqSJkpaKqlR0rQyx/tKmpOOPyppeO7YJSl9qaRTUtqwtLjO4rTe8Vdz+Q9IaxA/\nl37un9Il6epU1jMly3aaWRU5YOyduvLv1mHQSM/AvxY4lWzpyXMk1Zdkm0q2CtrhwJXAFencerL1\nhkcBE4HrUnnNwD+nRVmOBy7MlTkN+GVEjCRbf7g1SJ1Ktq7xSOB8siU4d4uX12/ie/ctZdnaDbvr\nEmZme6UiPY2xQGNELE/P7Z9NtvJX3iTeWUBnHjA+rSo2CZgdEZsj4nmydQjGRsTqiPgjbH9G/xLe\nWbIyX9YtZIvVt6bfGplHgIGShnSyvYWseWMTVz/YyAtNGzvObGZVd9VVV/GBD3yAUaNG8f3vf397\n+muvvcaECRMYOXIkEyZMYN26dQD8+te/5ve/f2ftqvPOO4958+bt8Xp31V133cXixYu370+fPp0H\nHngAgHHjxu3WLzUXCRoH03bd4ZW0XZO4TZ6IaAbWA4OKnJuGsj4EPJqSDoyI1Wn7ZbI1hovWA0nn\nS1ogacHatWs7bp2Z7dUWLlzIjTfeyGOPPcbTTz/NPffcQ2NjIwCXX34548eP57nnnmP8+PFcfvnl\nwI5BY09obm7eZWWVBo0ZM2Zw8skn77Ly21PViXBJ+5Kt4/y1iHij9Hhkj+Dt1GN4I+KGiBgTEWPq\n6so+OqUTZe3U6Wa2ByxZsoQPf/jDvOtd76JXr16cdNJJ/OxnPwPg7rvvZsqUKQBMmTKFu+66ixUr\nVnD99ddz5ZVXMnr0aB5++GEAHnroIT7ykY9w2GGHle11rFixgve///185jOf4aijjuKss87irbfe\nAuCJJ57gpJNO4rjjjuOUU05h9ers795x48bxta99jTFjxnDVVVexZs0a/u7v/o5jjjmGY445Znvg\n+vGPf8zYsWMZPXo0X/ziF2lpaQFg33335dJLL+WYY47h+OOPZ82aNfz+97+noaGBf/3Xf2X06NEs\nW7asYk/pvvvu44QTTuDYY4/l7LPPZsOGnR9yL3LL7SraLnQ/lLYL2efzrJTUi2zB+qb2zk1rHP8U\nuD0ifpbLs0bSkIhYnYafXulEPXYJz+mZdc0vfvELXn755V1a5nvf+14mTpxY8fgHPvABLr30Upqa\nmthnn32YP38+Y8aMAWDNmjUMGTJkezlr1qxh+PDhXHDBBey77778y7/8CwCzZs1i9erV/Pa3v+XZ\nZ5/ljDPO4KyzztrhWkuXLmXWrFl89KMf5Qtf+ALXXXcdX/3qV/nKV77C3XffTV1dHXPmzOHSSy/l\npptuAmDLli3bh4s+/elPc9JJJ3HnnXfS0tLChg0bWLJkCXPmzOF3v/sdvXv35stf/jK333475557\nLhs3buT4449n5syZfP3rX+fGG2/k3/7t3zjjjDM47bTTytax1auvvspll13GAw88QP/+/bniiiv4\n3ve+x/Tp07v2D5EUCRqPAyMljSD7kJ5Mtt5yXgMwBfgDcBbwYESEpAbgJ5K+BxxENon9WJrvmAUs\niYjvVSjr8vTz7lz6RZJmk609vD43jLVbuKdh1v0dddRRXHzxxXziE5+gf//+jB49mtra2h3ySWr3\nbqEzzzyTmpoa6uvrWbNmTdk8w4YN46Mf/SgAn/3sZ7n66quZOHEiCxcuZMKECQC0tLRsD1SQBYpW\nDz74ILfeeisAtbW1DBgwgNtuu40nnniCv/mbvwHg7bff5j3veQ8Affr04bTTTgPguOOO4/777y/8\ne3nkkUdYvHjx9vpu2bKFE044ofD5lXQYNCKiWdJFwL1ALXBTRCySNANYEBENZAHgNkmNwGtkgYWU\nby7ZAvbNwIUR0SLpROBzwJ8kPZUu9Y2ImE8WLOZKmgq8APxDOj4f+CTZZPpbwOd3uvUVCHc1zLqi\nvR7B7jR16lSmTp0KwDe+8Q2GDh0KwIEHHsjq1asZMmQIq1ev3v5hXE7fvn23b1danK406EgiIhg1\nahR/+MMfyp7Tv3//duseEUyZMoVvf/vbOxzr3bv39mvW1tZ2al4kIpgwYQJ33HFH4XOKKDSnERHz\nI+KIiHhfRMxMadNTwCAiNkXE2RFxeESMjYjluXNnpvOOjIifp7TfRoQi4uiIGJ1e89OxpogYHxEj\nI+LkiHgtpUdEXJjK+mBE9LxnnptZl7zySjaK/eKLL/Kzn/2Mf/zHbDDkjDPO4JZbspsxb7nlFiZN\nym783G+//XjzzTc7fZ0XX3xxe3D4yU9+woknnsiRRx7J2rVrt6dv3bqVRYsWlT1//Pjx/PCH2bcF\nWlpaWL9+PePHj2fevHnb2/Daa6/xwgsVn0xeuP7HH388v/vd77bfFLBx40b+/Oc/F29sBf5GeDs8\nOmW2d/j7v/976uvrOf3007n22msZOHAgANOmTeP+++9n5MiRPPDAA0ybln3t6/TTT+fOO+9sMxFe\nxJFHHsm1117LUUcdxbp16/jSl75Enz59mDdvHhdffDHHHHMMo0ePrnhn1lVXXcWvfvUrPvjBD3Lc\nccexePFi6uvrueyyy/jEJz7B0UcfzYQJE7ZPpFcyefJkvvvd7/KhD32IZcuWlc1TV1fHzTffzDnn\nnMPRRx/NCSecwLPPPlu4rZX06DXCx4wZE125X3nhqvWc9oPfcuO5Y5hQf2DHJ+xhXoTJupMlS5Zw\n1FFHVbsau92KFSs47bTTWLhwYbWrskuV+/eT9EREjCmX3z2NdvTkgGpm1hUOGmZmBQwfPrzH9TK6\nwkHDzHaae+V7p678uzlotMP/Dcw61q9fP5qamhw49jKt62n069evU+d5EaYy/I1ws+KGDh3KypUr\n8bPe9j6tK/d1hoNGO/yHk1nHevfu3amV32zv5uGpMvyNcDOz8hw0zMysMAeNdnl8yswsz0GjDE+E\nm5mV56BhZmaFOWi0w3dPmZm15aBRhoenzMzKKxQ0JE2UtFRSo6RpZY73lTQnHX9U0vDcsUtS+lJJ\np+TSb5L0iqSFJWXNkfRUeq1oXaRJ0nBJb+eOXd/VRhfljoaZWVsdfrlPUi1wLTABWAk8LqkhIhbn\nsk0F1kXE4ZImA1cAn5ZUT7aK3yiy5V4fkHRERLQANwPXALfmrxcR29dGlPS/gPW5w8siYnTnm9k5\n/p6GmVl5RXoaY4HGiFgeEVuA2cCkkjyTgFvS9jxgfFoHfBIwOyI2R8TzZEu1jgWIiIfIloYtK53/\nD8CuXavQzMy6rEjQOBh4Kbe/MqWVzRMRzWS9g0EFz63kY8CaiHgulzZC0pOSfiPpYwXL6TJPhJuZ\ntdWdnz11Dm17GauBQyKiSdJxwF2SRkXEG/mTJJ0PnA9wyCGHdOnCngg3MyuvSE9jFTAstz80pZXN\nI6kXMABoKnjuDlIZ/xmY05qWhria0vYTwDLgiNJzI+KGiBgTEWPq6uo6bFx7wlPhZmZtFAkajwMj\nJY2Q1IdsYruhJE8DMCVtnwU8GNnD9RuAyenuqhHASOCxAtc8GXg2Ila2JkiqS5PySDoslbW8QFmd\n5o6GmVl5HQ5PRUSzpIuAe4Fa4KaIWCRpBrAgIhqAWcBtkhrJJrcnp3MXSZoLLAaagQvTnVNIugMY\nBwyWtBL4ZkTMSpedzI4T4H8LzJC0FdgGXBARFSfSzcxs1ys0pxER84H5JWnTc9ubgLMrnDsTmFkm\n/Zx2rndembSfAj8tUt9dxRPhZmZt+RvhZXgi3MysPAeNdrijYWbWloNGWe5qmJmV46BhZmaFOWi0\nIzwTbmbWhoNGGZ4INzMrz0HDzMwKc9Aowx0NM7PyHDTMzKwwB412eB7czKwtB40y5JlwM7OyHDTM\nzKwwB412eD0NM7O2HDTK8OCUmVl5Dhrt8ES4mVlbDhpleB7czKy8QkFD0kRJSyU1SppW5nhfSXPS\n8UclDc8duySlL5V0Si79JkmvSFpYUta3JK2S9FR6fbKjsszMbM/oMGikdbmvBU4F6oFzJNWXZJsK\nrIuIw4ErgSvSufVkS7eOAiYC17Wu8w3cnNLKuTIiRqfX/AJl7RYenjIza6tIT2Ms0BgRyyNiCzAb\nmFSSZxJwS9qeB4xX9mWHScDsiNgcEc8Djak8IuIhsvXEi6pY1q4mT4WbmZVVJGgcDLyU21+Z0srm\niYhmYD0wqOC55Vwk6Zk0hLV/J+qBpPMlLZC0YO3atQUuVZk7GmZmbXXHifAfAu8DRgOrgf/VmZMj\n4oaIGBMRY+rq6rpUAU+Em5mVVyRorAKG5faHprSyeST1AgYATQXPbSMi1kRES0RsA27knSGoTpdl\nZma7VpGg8TgwUtIISX3IJqMbSvI0AFPS9lnAg5Ete9cATE53V40ARgKPtXcxSUNyu38HtN5d1emy\ndpZX7jMza6tXRxkiolnSRcC9QC1wU0QskjQDWBARDcAs4DZJjWST25PTuYskzQUWA83AhRHRAiDp\nDmAcMFjSSuCbETEL+I6k0WRTCiuAL3ZUlpmZ7RkdBg2AdNvr/JK06bntTcDZFc6dCcwsk35Ohfyf\na6ceZcvaXdzPMDNrqztOhFedJ8LNzMpz0DAzs8IcNNrj8SkzszYcNMrwyn1mZuU5aJiZWWEOGu3w\nyn1mZm05aJThwSkzs/IcNNrhL4SbmbXloFGG58HNzMpz0DAzs8IcNNrh0Skzs7YcNMrwyn1mZuU5\naLTDE+FmZm05aJThiXAzs/IcNMzMrDAHjXb4G+FmZm0VChqSJkpaKqlR0rQyx/tKmpOOPyppeO7Y\nJSl9qaRTcuk3SXpF0sKSsr4r6VlJz0i6U9LAlD5c0tuSnkqv67va6A7bu7sKNjPby3UYNCTVAtcC\npwL1wDmS6kuyTQXWRcThwJXAFencerKlX0cBE4HrUnkAN6e0UvcDH4iIo4E/A5fkji2LiNHpdUGx\nJnadJ8LNzNoq0tMYCzRGxPKI2ALMBiaV5JkE3JK25wHjlT1ffBIwOyI2R8TzQGMqj4h4iGw98TYi\n4r6IaE67jwBDO9mmneeuhplZWUWCxsHAS7n9lSmtbJ70gb8eGFTw3PZ8Afh5bn+EpCcl/UbSx8qd\nIOl8SQskLVi7dm0nLmVmZh3pthPhki4FmoHbU9Jq4JCI+BDwT8BPJL279LyIuCEixkTEmLq6up2q\ng0enzMzaKhI0VgHDcvtDU1rZPJJ6AQOApoLn7kDSecBpwGcispmFNMTVlLafAJYBRxSof6f5G+Fm\nZuUVCRqPAyMljZDUh2xiu6EkTwMwJW2fBTyYPuwbgMnp7qoRwEjgsfYuJmki8HXgjIh4K5de1zqJ\nLumwVNbyAvU3M7NdpFdHGSKiWdJFwL1ALXBTRCySNANYEBENwCzgNkmNZJPbk9O5iyTNBRaTDTVd\nGBEtAJLuAMYBgyWtBL4ZEbOAa4C+wP1pre5H0p1SfwvMkLQV2AZcEBE7TKTvUr59ysysjQ6DBkBE\nzAfml6RNz21vAs6ucO5MYGaZ9HMq5D+8QvpPgZ8Wqe/O8mNEzMzK67YT4d2B+xlmZm05aJThjoaZ\nWXkOGmZmVpiDRjs8D25m1paDRhnyTLiZWVkOGu0IdzXMzNpw0CijJnU0HDLMzNpy0CijdXhqm6OG\nmVkbDhplbO9peHjKzKwNB40yarb3NBw0zMzyHDTKqPHwlJlZWQ4aZbTeceuehplZWw4aZbT2NBwz\nzMzactAoo3UifJvHp8zM2nDQKMNzGmZm5RUKGpImSloqqVHStDLH+0qak44/Kml47tglKX2ppFNy\n6TdJekXSwpKyDpB0v6Tn0s/9U7okXZ3KekbSsV1tdMftzX56TsPMrK0Og0ZaYvVa4FSgHjhHUn1J\ntqnAurSA0pXAFencerJV/EYBE4HrWpdsBW5OaaWmAb+MiJHAL9M+6foj0+t84IfFmth52j6n4aBh\nZpZXpKcxFmiMiOURsQWYDUwqyTMJuCVtzwPGK/vknQTMjojNEfE80JjKIyIeIlsatlS+rFuAM3Pp\nt0bmEWCgpCFFGtkVNfJjRMzMShUJGgcDL+X2V6a0snkiohlYDwwqeG6pAyNiddp+GTiwE/VA0vmS\nFkhasHbt2g4uVZkkD0+ZmZXo1hPhkY0PdeqTOyJuiIgxETGmrq6uy9cWngg3MytVJGisAobl9oem\ntLJ5JPUCBgBNBc8ttaZ12Cn9fKUT9dhlatzTMDPbQZGg8TgwUtIISX3IJrYbSvI0AFPS9lnAg6mX\n0ABMTndXjSCbxH6sg+vly5oC3J1LPzfdRXU8sD43jLXLSf5yn5lZqV4dZYiIZkkXAfcCtcBNEbFI\n0gxgQUQ0ALOA2yQ1kk1uT07nLpI0F1gMNAMXRkQLgKQ7gHHAYEkrgW9GxCzgcmCupKnAC8A/pKrM\nBz5JNpn+FvD5XfELqETyl/vMzEp1GDQAImI+2Yd2Pm16bnsTcHaFc2cCM8ukn1MhfxMwvkx6ABcW\nqe+uIESLuxpmZm1064nwaqpxT8PMbAcOGhVI7mmYmZVy0KigRtCyrdq1MDPrXhw0KqiRPDxlZlbC\nQaMCCZodNMzM2nDQqMBf7jMz25GDRgU1Ei3uaZiZteGgUUFNDQ4aZmYlHDQqkHsaZmY7cNCoQODv\naZiZlXDQqMDPnjIz25GDRgVCvuXWzKyEg0YFEr7l1syshINGBcJ3T5mZlXLQqEBy0DAzK+WgUYFv\nuTUz21GhoCFpoqSlkholTStzvK+kOen4o5KG545dktKXSjqlozIlPSzpqfT6i6S7Uvo4Setzx6az\nGwlo3ubH3JqZ5XW4cp+kWuBaYAKwEnhcUkNELM5lmwqsi4jDJU0GrgA+LamebOnXUcBBwAOSjkjn\nlC0zIj6Wu/ZPeWeNcICHI+K0rja2M2pqRHOLexpmZnlFehpjgcaIWB4RW4DZwKSSPJOAW9L2PGC8\nJKX02RGxOSKeJ1vfe2yRMiW9G/g4cFfXmrZzaiW2enjKzKyNIkHjYOCl3P7KlFY2T0Q0A+uBQe2c\nW6TMM4FfRsQbubQTJD0t6eeSRpWrrKTzJS2QtGDt2rUFmldejcTWZg9PmZnldeeJ8HOAO3L7fwQO\njYhjgB9QoQcSETdExJiIGFNXV9fli9fWiK2e0zAza6NI0FgFDMvtD01pZfNI6gUMAJraObfdMiUN\nJhvC+j+taRHxRkRsSNvzgd4p325RU4PnNMzMShQJGo8DIyWNkNSHbGK7oSRPAzAlbZ8FPBgRkdIn\np7urRgAjgccKlHkWcE9EbGpNkPTeNE+CpLGp7k2da25xtRJbvUi4mVkbHd49FRHNki4C7gVqgZsi\nYpGkGcCCiGgAZgG3SWoEXiMLAqR8c4HFQDNwYUS0AJQrM3fZycDlJVU5C/iSpGbgbWByCky7he+e\nMjPbUYdBA7YPB80vSZue294EnF3h3JnAzCJl5o6NK5N2DXBNkfruCu5pmJntqDtPhFdVbY2fcmtm\nVspBowIHDTOzHTloVFCTHli4G6dNzMz2Og4aFdTWZL+aLZ7XMDPbzkGjgl41AmDTVgcNM7NWDhoV\n9K7NfjWbtrZUuSZmZt2Hg0YFvWpbexoOGmZmrRw0Kuidhqfe3NRc5ZqYmXUfDhqVZDGDzc3uaZiZ\ntXLQqKB/n+zL8m9v8US4mVkrB40KWifC39ri4Skzs1YOGhX07tUaNDw8ZWbWykGjgj7p7qmN7mmY\nmW3noFFB3161AGzw3VNmZts5aFTQu9a33JqZlXLQqEASNYINmx00zMxaFQoakiZKWiqpUdK0Msf7\nSpqTjj8qaXju2CUpfamkUzoqU9LNkp6X9FR6jU7pknR1yv+MpGN3puFFSHJPw8wsp8OgIakWuBY4\nFagHzpFUX5JtKrAuIg4HrgSuSOfWky3dOgqYCFwnqbZAmf8aEaPT66mUdirZGuMjgfOBH3alwZ31\n9lYHDTOzVkV6GmOBxohYHhFbgNnApJI8k4Bb0vY8YLwkpfTZEbE5Ip4HGlN5RcosNQm4NTKPAAMl\nDSlQ/y6LCN9ya2aWUyRoHAy8lNtfmdLK5omIZmA9MKidczsqc2YagrpSUt9O1ANJ50taIGnB2rVr\nCzSvsm3hOQ0zs7zuOBF+CfB+4G+AA4CLO3NyRNwQEWMiYkxdXd1OV+aNt7fudBlmZj1FkaCxChiW\n2x+a0srmkdQLGAA0tXNuxTIjYnUagtoM/P9kQ1lF67HLvf6Wg4aZWasiQeNxYKSkEZL6kE1sN5Tk\naQCmpO2zgAcjW1y7AZic7q4aQTaJ/Vh7ZbbOU6Q5kTOBhblrnJvuojoeWB8Rq7vU6k7w3VNmZu/o\n1VGGiGiWdBFwL1AL3BQRiyTNABZERAMwC7hNUiPwGlkQIOWbCywGmoELI6IFoFyZ6ZK3S6ojezj5\nU8AFKX0+8EmyyfS3gM/vdOsLeHtrC1tbtm1/gKGZ2V+zDoMGQETMJ/vQzqdNz21vAs6ucO5MYGaR\nMlP6xyuUE8CFReq7q725qZkD+vepxqXNzLoV//lcwHpPhpuZAQ4ahby2cXO1q2Bm1i04aBTwyhsO\nGmZm4KBRyNoNDhpmZuCgUcirG7ZUuwpmZt2Cg0YHagUvr3+72tUwM+sWHDQ6EMDq9ZuqXQ0zs27B\nQaMD2wJWrnNPw8wMHDQKef7VjWTfLTQz++vmoFGQJ8PNzBw0Cntp3VvVroKZWdU5aBT0QtPGalfB\nzKzqHDQKev5V9zTMzBw0CtW2ef4AAAwNSURBVKiVeHb1G9WuhplZ1TloFNASwRMvrKt2NczMqq5Q\n0JA0UdJSSY2SppU53lfSnHT8UUnDc8cuSelLJZ3SUZmSbk/pCyXdJKl3Sh8nab2kp9JrOntQ08Yt\nNPkZVGb2V67DoCGpFrgWOBWoB86RVF+SbSqwLiIOB64Erkjn1pOt4jcKmAhcJ6m2gzJvB94PfBDY\nB/gvues8HBGj02tGVxq8M/60av2evqSZWbdSpKcxFmiMiOURsQWYDUwqyTMJuCVtzwPGpzW+JwGz\nI2JzRDxPtlTr2PbKjIj5kZCtJz5055q46/zRQ1Rm9leuSNA4GHgpt78ypZXNExHNwHpgUDvndlhm\nGpb6HPCLXPIJkp6W9HNJo8pVVtL5khZIWrB27doCzStGwGMrXttl5ZmZ7Y2680T4dcBDEfFw2v8j\ncGhEHAP8ALir3EkRcUNEjImIMXV1dbusMgH88cXX2dK8bZeVaWa2tykSNFYBw3L7Q1Na2TySegED\ngKZ2zm23TEnfBOqAf2pNi4g3ImJD2p4P9JY0uED9d5ktzdt48kUPUZnZX68iQeNxYKSkEZL6kE1s\nN5TkaQCmpO2zgAfTnEQDMDndXTUCGEk2T1GxTEn/BTgFOCcitv9ZL+m9aZ4ESWNT3Zu60uiuEnDf\n4jV78pJmZt1Kh0EjzVFcBNwLLAHmRsQiSTMknZGyzQIGSWok6x1MS+cuAuYCi8nmJi6MiJZKZaay\nrgcOBP5QcmvtWcBCSU8DVwOTYw8/ejaAe575C80tHqIys79OvYpkSsNB80vSpue2NwFnVzh3JjCz\nSJkpvWydIuIa4Joi9d2d1ryxmYeeW8vH339gtatiZrbHdeeJ8G6pRnDrH16odjXMzKrCQaOTtgX8\neulalq3dUO2qmJntcQ4aXSDgRw8/X+1qmJntcQ4aXRDA7Mdf5IkX/GU/M/vr4qDRRQK+Pu8ZNmxu\nrnZVzMz2GAeNLtoWsGztRv5pzlNsbm6pdnXMzPYIB42ddN/iNZw76zHWbdxS7aqYme12Dhq7wGPP\nv8aEK3/DnU+uZNu2Pfp9QzOzPcpBYxcIskWa/vucpzn5yt/wv5/+Cy0OHmbWAzlo7CKtDzRZ8epG\nvnLHk5x4xYNc/vNnWfSX9ezhp52Yme02hR4jYsW1djBeXr+J63+zjOt/s4zD6vpzxjEHcfoxB/G+\nun2rW0Ezs53goLGb5PsWLzS9xfcfeI7vP/Ac73/vfvz9sUM5Y/RBHPjuflWrn5lZVzho7AH5+Y2l\nL7/JzPlL+J/zl/ChQwZycv2B/O3IOuqHvJuaGlWxlmZmHXPQ2MMi9/Opl17njy++znd+sZSB+/Tm\nxJGDGT1sIKOHDeQDBw+gX+/aalbVzGwHDhpVlL/B6vW3t/LzhS9zzzOrgexpuoe/Z1+OOHA/Dhvc\nn+GD+zNicH8OG+w5ETOrHgeNbiQ/jLUt4M9rNrB87UaaS27fPW2fdezTu5Z/nvs0h9VlwWTIgH4c\n0L8P+/fvw359e5EWOTQz26UKBQ1JE4GrgFrgRxFxecnxvsCtwHFkS7B+OiJWpGOXAFOBFuC/RcS9\n7ZWZloWdDQwCngA+FxFb2rtGT1YaMACaW7axYds27n9qVdnjtRKD9u3D8MH9GbxvHwbs04f939Wb\nge/qzaD+fXn3Pr0ZMqAfA/bpzT59aulVI2pqlP2UqE3bDjxmVqrDoCGpFrgWmACsBB6X1BARi3PZ\npgLrIuJwSZOBK4BPS6onW/97FHAQ8ICkI9I5lcq8ArgyImZLuj6V/cNK19jZX8DeKqJ8QAFoieCV\nNzfzypubqVVrWufKl2BAv97s268XfWpr6NOrhkH9+9C3dy01gn369OJ9df2pkVDKL4nTjh7CoYP6\n71zjzKzbKtLTGAs0RsRyAEmzgUlk6363mgR8K23PA65R9mfqJGB2RGwGnk9riI9N+XYoU9IS4OPA\nP6Y8t6Ryf1jpGrtrnfB+vWs5oH+f3VH0Tuu9LftOZmfqFxEEWbCJCLbFO2mQJuhzv8kgeHtrC29t\naaE117K1rWVlQ2nlfvHfvXcpI9/jeRezaht3ZB2Xfqp+l5dbJGgcDLyU218JfLhSnoholrSebHjp\nYOCRknMPTtvlyhwEvB4RzWXyV7rGq/mKSDofOB/gkEMOKdC88qZ8ZDhTPjK8y+fvTjffvAqAH5w3\noWp12LatNQi9E4zmLHiJPyx7taNTzWwP2F3fA+txE+ERcQNwA8CYMWP8/I7d5J3vlLwz7/G54w/l\nc8cfWp0KmdkeUeTZU6uAYbn9oSmtbB5JvYABZJPVlc6tlN4EDExllF6r0jXMzGwPKRI0HgdGShoh\nqQ/ZxHZDSZ4GYEraPgt4MM01NACTJfVNd0WNBB6rVGY651epDFKZd3dwDTMz20M6HJ5K8wcXAfeS\n3R57U0QskjQDWBARDcAs4LY00f0aWRAg5ZtLNmneDFwYES0A5cpMl7wYmC3pMuDJVDaVrmFmZntO\noTmNiJgPzC9Jm57b3gScXeHcmcDMImWm9OW8c4dVPr3iNczMbM/wehpmZlaYg4aZmRXmoGFmZoU5\naJiZWWHqyXetSloLvLATRQym5BvnPUhPbhu4fXuzntw22Dvad2hE1JU70KODxs6StCAixlS7HrtD\nT24buH17s57cNtj72+fhKTMzK8xBw8zMCnPQaN8N1a7AbtST2wZu396sJ7cN9vL2eU7DzMwKc0/D\nzMwKc9AwM7PCHDTKkDRR0lJJjZKmVbs+7ZF0k6RXJC3MpR0g6X5Jz6Wf+6d0Sbo6tesZScfmzpmS\n8j8naUou/ThJf0rnXJ2W8d1TbRsm6VeSFktaJOmrPax9/SQ9Junp1L5/T+kjJD2a6jQnLR9AWmJg\nTkp/VNLwXFmXpPSlkk7JpVf1vSypVtKTku7pgW1bkd47T0lakNJ6xHuzXRHhV+5F9qj2ZcBhQB/g\naaC+2vVqp75/CxwLLMylfQeYlranAVek7U8CPydbbu944NGUfgCwPP3cP23vn449lvIqnXvqHmzb\nEODYtL0f8Gegvge1T8C+abs38Giqy1xgckq/HvhS2v4ycH3angzMSdv16X3aFxiR3r+13eG9DPwT\n8BPgnrTfk9q2AhhcktYj3pvtvdzT2NFYoDEilkfEFmA2MKnKdaooIh4iW18kbxJwS9q+BTgzl35r\nZB4hWyVxCHAKcH9EvBYR64D7gYnp2Lsj4pHI3sW35sra7SJidUT8MW2/CSwhWyu+p7QvImJD2u2d\nXgF8HJiX0kvb19ruecD49NfnJGB2RGyOiOeBRrL3cVXfy5KGAp8CfpT2RQ9pWzt6xHuzPQ4aOzoY\neCm3vzKl7U0OjIjVaftl4MC0Xalt7aWvLJO+x6Xhig+R/TXeY9qXhm+eAl4h+8BYBrweEc1l6rS9\nHen4emAQnW/3nvJ94OvAtrQ/iJ7TNsgC/H2SnpB0fkrrMe/NSgotwmR7r4gISXv1fdWS9gV+Cnwt\nIt7ID+3u7e2LbCXL0ZIGAncC769ylXYJSacBr0TEE5LGVbs+u8mJEbFK0nuA+yU9mz+4t783K3FP\nY0ergGG5/aEpbW+yJnVvST9fSemV2tZe+tAy6XuMpN5kAeP2iPhZSu4x7WsVEa8DvwJOIBu6aP2D\nLl+n7e1IxwcATXS+3XvCR4EzJK0gGzr6OHAVPaNtAETEqvTzFbKAP5Ye+N7cQbUnVbrbi6z3tZxs\n0q11gm1UtevVQZ2H03Yi/Lu0nYz7Ttr+FG0n4x5L6QcAz5NNxO2ftg9Ix0on4z65B9slsrHc75ek\n95T21QED0/Y+wMPAacB/0Hay+Mtp+0LaThbPTdujaDtZvJxsorhbvJeBcbwzEd4j2gb0B/bLbf8e\nmNhT3pvttr3aFeiOL7I7Hf5MNr58abXr00Fd7wBWA1vJxj2nko0F/xJ4Dngg9yYUcG1q15+AMbly\nvkA2ydgIfD6XPgZYmM65hvQUgT3UthPJxo2fAZ5Kr0/2oPYdDTyZ2rcQmJ7SD0sfGI3pQ7ZvSu+X\n9hvT8cNyZV2a2rCU3F023eG9TNug0SPaltrxdHotar1+T3lvtvfyY0TMzKwwz2mYmVlhDhpmZlaY\ng4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFfZ/AcJBV60Wxa1NAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1WhlkVVv_Sp",
        "colab_type": "text"
      },
      "source": [
        "Above is shown the variable importance scores, sorted, for all 54,623 variables. As we can see, the vast majority of the variable importance is held within the top several thousand variables, with the vertical line indicating the 90th percentile mark.\n",
        "\n",
        "Below I extract the 500, 1000, and 2000 most important features (as determined by the above score)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUsnZWuhQPb2",
        "colab_type": "code",
        "outputId": "66b82979-c339-4d30-b0ac-c607325aded8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# extract most important features\n",
        "var_import_index = np.argsort(rf_mod.feature_importances_)[::-1]   # sort by feature importance\n",
        "data_best500 = data.iloc[:,var_import_index[:500]]   # top 500 features\n",
        "data_best1000 = data.iloc[:,var_import_index[:1000]]    # top 1000\n",
        "data_best2000 = data.iloc[:,var_import_index[:2000]]    # top 2000\n",
        "print(data_best500.iloc[:5,:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               PAX8   TMEM101     RGS20    TRIM15     PRSS3\n",
            "GSM803615 -0.385425  0.041156 -1.224200 -0.149815 -0.644826\n",
            "GSM803674 -0.534277 -0.070203 -1.493759 -0.510926 -0.432550\n",
            "GSM803733 -0.372903  0.010249 -0.781479 -0.638624 -0.517498\n",
            "GSM803616 -0.674562  0.646346 -1.378388 -0.425276 -0.299458\n",
            "GSM803675 -0.333305  0.490886 -1.156327  0.423250 -0.348551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXmnbAJoxUmk",
        "colab_type": "text"
      },
      "source": [
        "A quick google of these genes is very encouraging. Acording to NCBI.gov: \"Mutations in the PAX8 gene have been associated with thyroid dysgenesis, thyroid follicular carcinomas and atypical follicular thyroid adenomas.\" The top gene is associated with thyroid cancer! Below I plot the distributions of a few of these variables accross the 9 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OenjD1I6Thq-",
        "colab_type": "code",
        "outputId": "969b606f-cc10-4be8-bf03-9fdab8d222de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "fig, axes = plt.subplots(ncols=3, sharey=True, figsize=(16,6),)\n",
        "pax8_box = [data_best500[labels==i].iloc[:,0] for i in range(9)]\n",
        "tmem101_box = [data_best500[labels==i].iloc[:,1] for i in range(9)]\n",
        "trim15_box = [data_best500[labels==i].iloc[:,3] for i in range(9)]\n",
        "_= axes[0].boxplot(pax8_box)\n",
        "axes[0].set_title(\"PAX8\")\n",
        "_=axes[1].boxplot(tmem101_box)\n",
        "axes[1].set_title(\"TMEM101\")\n",
        "_=axes[2].boxplot(trim15_box)\n",
        "_=axes[2].set_title(\"TRIM15\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6AAAAF1CAYAAAD2qYcEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAevH/A39diVlfq6uXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1152x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ3F6PWVgY3d",
        "colab_type": "text"
      },
      "source": [
        "These variables with high importance are clearly very useful in separating the classes. PAX8 has high expression in classes 7 and 9, TMEM101 has low expression for class 7, and TRIM15 has high expression in class 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo3OmfEXfWwA",
        "colab_type": "text"
      },
      "source": [
        "## Boosting with Variable Selection\n",
        "Now I will attempt the same grid search as above, w/ 5 fold CV, using only the top 500 variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr9FXEpZUVOy",
        "colab_type": "code",
        "outputId": "f295acdc-ea39-4b3e-cdc1-01b254d7cbd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Boosting 500 features, 5-fold CV, Grid search\n",
        "random.seed(711)\n",
        "n_trees = np.array([100, 500, 900])  # grid search parameters\n",
        "rate = np.array([.1, .01, .001])\n",
        "depth = np.array([2, 4])\n",
        "params = np.array(np.meshgrid(n_trees, rate, depth)).reshape(3,-1).T\n",
        "\n",
        "# 5-fold CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "for p in params:\n",
        "  boost_mod = GradientBoostingClassifier(n_estimators=int(p[0]), learning_rate=p[1], max_depth=int(p[2]))  # create boosting model\n",
        "  error = []\n",
        "  for train_index, test_index in skf.split(data_best500, labels):\n",
        "    X_train, X_test = data_best500.iloc[train_index], data_best500.iloc[test_index]  # train-test split\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    boost_mod.fit(X_train, y_train)    # fit on train\n",
        "    y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "    error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(sum(error)/len(error), p)  # average accuracy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9258928571428571 [100.    0.1   2. ]\n",
            "0.9296428571428571 [100.    0.1   4. ]\n",
            "0.9446428571428571 [5.e+02 1.e-01 2.e+00]\n",
            "0.925357142857143 [5.e+02 1.e-01 4.e+00]\n",
            "0.9473214285714284 [9.e+02 1.e-01 2.e+00]\n",
            "0.9248214285714285 [9.e+02 1.e-01 4.e+00]\n",
            "0.9225 [1.e+02 1.e-02 2.e+00]\n",
            "0.9242857142857144 [1.e+02 1.e-02 4.e+00]\n",
            "0.9473214285714284 [5.e+02 1.e-02 2.e+00]\n",
            "0.9560714285714287 [5.e+02 1.e-02 4.e+00]\n",
            "0.9217857142857143 [9.e+02 1.e-02 2.e+00]\n",
            "0.9466071428571429 [9.e+02 1.e-02 4.e+00]\n",
            "0.8448214285714286 [1.e+02 1.e-03 2.e+00]\n",
            "0.8566071428571428 [1.e+02 1.e-03 4.e+00]\n",
            "0.9473214285714284 [5.e+02 1.e-03 2.e+00]\n",
            "0.9216071428571428 [5.e+02 1.e-03 4.e+00]\n",
            "0.9226785714285715 [9.e+02 1.e-03 2.e+00]\n",
            "0.9253571428571428 [9.e+02 1.e-03 4.e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dhAlKBTztoV",
        "colab_type": "text"
      },
      "source": [
        "This code ran much faster, but these results are not encouraging. The top score was 94% accuracy, which doesn't even beat Naive Bayes. Perhaps 500 variables are not enough. We attempt the same with 1000 variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whxTcRp-kcGI",
        "colab_type": "code",
        "outputId": "38019ad2-5d89-4f90-d5a2-dbe4ce4350d4",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#@title Boosting, top 1000 features\n",
        "# Boosting after feature selection, 5-fold CV, Grid search, best 1000\n",
        "random.seed(711)\n",
        "n_trees = np.array([100, 500, 900])  # grid search parameters\n",
        "rate = np.array([.1, .01, .001])\n",
        "depth = np.array([2, 4])\n",
        "params = np.array(np.meshgrid(n_trees, rate, depth)).reshape(3,-1).T\n",
        "\n",
        "# 5-fold CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "for p in params:\n",
        "  boost_mod = GradientBoostingClassifier(n_estimators=int(p[0]), learning_rate=p[1], max_depth=int(p[2]))  # create boosting model\n",
        "  error = []\n",
        "  for train_index, test_index in skf.split(data_best1000, labels):\n",
        "    X_train, X_test = data_best1000.iloc[train_index], data_best1000.iloc[test_index]  # train-test split\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    boost_mod.fit(X_train, y_train)    # fit on train\n",
        "    y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "    error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(sum(error)/len(error), p)  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9535714285714285 [100.    0.1   2. ]\n",
            "0.9428571428571428 [100.    0.1   4. ]\n",
            "0.9724999999999999 [5.e+02 1.e-01 2.e+00]\n",
            "0.8914285714285715 [5.e+02 1.e-01 4.e+00]\n",
            "0.98375 [9.e+02 1.e-01 2.e+00]\n",
            "0.9446428571428571 [9.e+02 1.e-01 4.e+00]\n",
            "0.9766071428571429 [1.e+02 1.e-02 2.e+00]\n",
            "0.9351785714285714 [1.e+02 1.e-02 4.e+00]\n",
            "0.9358928571428571 [5.e+02 1.e-02 2.e+00]\n",
            "0.9221428571428572 [5.e+02 1.e-02 4.e+00]\n",
            "0.9116071428571428 [9.e+02 1.e-02 2.e+00]\n",
            "0.9419642857142858 [9.e+02 1.e-02 4.e+00]\n",
            "0.8487500000000001 [1.e+02 1.e-03 2.e+00]\n",
            "0.8491071428571428 [1.e+02 1.e-03 4.e+00]\n",
            "0.9321428571428572 [5.e+02 1.e-03 2.e+00]\n",
            "0.90625 [5.e+02 1.e-03 4.e+00]\n",
            "0.9546428571428571 [9.e+02 1.e-03 2.e+00]\n",
            "0.91375 [9.e+02 1.e-03 4.e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_O_MyCc0Xsa",
        "colab_type": "text"
      },
      "source": [
        "This is better. The best model had 98.3% accuracy. It's parameters were `[n_estimators=900, learning_rate=.1, max_depth=2] `. This is getting close to our Random Forest benchmark of 98.8%.\n",
        "\n",
        "I will now grid-search around these parameters: I will try `n_estimators=[800,1000,1200]` and `learning_rate=[.01,.1,.2]` with `max_depth=2` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj5PhuiXVeir",
        "colab_type": "code",
        "outputId": "68c4eac7-666c-4705-e4af-a3e962738388",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#@title Grid Search, top 1000 features\n",
        "# Boosting after feature selection, 5-fold CV, Grid search, best 1000\n",
        "random.seed(711)\n",
        "n_trees = np.array([800, 1000, 1200])  # grid search parameters\n",
        "rate = np.array([.05, .1, .2])\n",
        "depth = np.array([2])\n",
        "params = np.array(np.meshgrid(n_trees, rate, depth)).reshape(3,-1).T\n",
        "\n",
        "# 5-fold CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "for p in params:\n",
        "  boost_mod = GradientBoostingClassifier(n_estimators=int(p[0]), learning_rate=p[1], max_depth=int(p[2]))  # create boosting model\n",
        "  error = []\n",
        "  for train_index, test_index in skf.split(data_best, labels):\n",
        "    X_train, X_test = data_best.iloc[train_index], data_best.iloc[test_index]  # train-test split\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    boost_mod.fit(X_train, y_train)    # fit on train\n",
        "    y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "    error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(sum(error)/len(error), p)  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9435714285714285 [8.e+02 5.e-02 2.e+00]\n",
            "0.9580357142857142 [1.e+03 5.e-02 2.e+00]\n",
            "0.9305357142857144 [1.2e+03 5.0e-02 2.0e+00]\n",
            "0.9216071428571428 [8.e+02 1.e-01 2.e+00]\n",
            "0.9378571428571428 [1.e+03 1.e-01 2.e+00]\n",
            "0.9435714285714287 [1.2e+03 1.0e-01 2.0e+00]\n",
            "0.9335714285714285 [8.e+02 2.e-01 2.e+00]\n",
            "0.9428571428571428 [1.e+03 2.e-01 2.e+00]\n",
            "0.9321428571428572 [1.2e+03 2.0e-01 2.0e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTcL0T3X3ywH",
        "colab_type": "text"
      },
      "source": [
        "None of these settings improved performance. I will retry the original search parameters, but with 2000 variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noU6FsXAYkyH",
        "colab_type": "code",
        "outputId": "9b5f6c22-4415-4e7f-ddfe-0245af5ceedc",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#@title Boosting, top 2000 features\n",
        "# Boosting after feature selection, 5-fold CV, Grid search, best 2000\n",
        "random.seed(711)\n",
        "n_trees = np.array([100, 500, 900])  # grid search parameters\n",
        "rate = np.array([.1, .01, .001])\n",
        "depth = np.array([2, 4])\n",
        "params = np.array(np.meshgrid(n_trees, rate, depth)).reshape(3,-1).T\n",
        "\n",
        "# 5-fold CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "for p in params:\n",
        "  boost_mod = GradientBoostingClassifier(n_estimators=int(p[0]), learning_rate=p[1], max_depth=int(p[2]))  # create boosting model\n",
        "  error = []\n",
        "  for train_index, test_index in skf.split(data_best2000, labels):\n",
        "    X_train, X_test = data_best2000.iloc[train_index], data_best2000.iloc[test_index]  # train-test split\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    boost_mod.fit(X_train, y_train)    # fit on train\n",
        "    y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "    error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(sum(error)/len(error), p)  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9730357142857142 [100.    0.1   2. ]\n",
            "0.9135714285714286 [100.    0.1   4. ]\n",
            "0.8982142857142857 [5.e+02 1.e-01 2.e+00]\n",
            "0.95625 [5.e+02 1.e-01 4.e+00]\n",
            "0.9394642857142858 [9.e+02 1.e-01 2.e+00]\n",
            "0.9446428571428571 [9.e+02 1.e-01 4.e+00]\n",
            "0.9166071428571427 [1.e+02 1.e-02 2.e+00]\n",
            "0.9151785714285714 [1.e+02 1.e-02 4.e+00]\n",
            "0.9605357142857143 [5.e+02 1.e-02 2.e+00]\n",
            "0.9319642857142856 [5.e+02 1.e-02 4.e+00]\n",
            "0.9596428571428571 [9.e+02 1.e-02 2.e+00]\n",
            "0.9396428571428572 [9.e+02 1.e-02 4.e+00]\n",
            "0.8701785714285715 [1.e+02 1.e-03 2.e+00]\n",
            "0.8753571428571428 [1.e+02 1.e-03 4.e+00]\n",
            "0.9435714285714287 [5.e+02 1.e-03 2.e+00]\n",
            "0.90625 [5.e+02 1.e-03 4.e+00]\n",
            "0.91875 [9.e+02 1.e-03 2.e+00]\n",
            "0.918392857142857 [9.e+02 1.e-03 4.e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02ZOOzwf4VeD",
        "colab_type": "text"
      },
      "source": [
        "None of these models outperformed the models with only 1000 features, and this is beggining to take significantly more time to run. We will try to optimize the approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz_tiF5FdVj6",
        "colab_type": "text"
      },
      "source": [
        "## XGBoost\n",
        "XGBoost is the python industry standard for boosted trees, with several useful features including stochastic gradient boosting and GPU acceleration. This will allow me to run models faster.\n",
        "\n",
        "Another important method used often in practice is Random Search (as opposed to Grid Search). It is more thoughrough, and can efficiently explore more of the parameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIYd2CZ1AW8v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1WIqueGgaqcXRMWBcljgrMd10IBnluKQN\"  width= \"700\"> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXczUnoPju_c",
        "colab_type": "text"
      },
      "source": [
        "*With grid search, nine trials only test three distinct places in the important parameter. With random search, all nine trails explore distinct values. Figure Credit: Peter Worcester*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8BuUQaWGQXG",
        "colab_type": "text"
      },
      "source": [
        "I will attempt a RandomSearch through the parameters, and will attempt to use the stochastic boosting feature. This means that only a random portion of the variables and datapoints are used to fit each small tree. For this trial I will use 10% of the rows and 10% of the columns for stochastic boosting. This will allow the models to run in reasonable time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVdlr-_gdg_L",
        "colab_type": "code",
        "outputId": "ec31c251-6a34-4837-c918-2716e11728b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#XGboost, RandomSearch, stochastic gradient\n",
        "random.seed(711)\n",
        "n_iter = 10\n",
        "\n",
        "n_trees = np.array([100, 200, 300, 500, 700])  \n",
        "rate = np.array([0.1, .07, .05, .03, 0.01])\n",
        "depth = np.array([2,3,4])\n",
        "params = np.array([np.random.choice(n_trees,n_iter),np.random.choice(rate,n_iter),np.random.choice(depth,n_iter)]).T  # grid search params\n",
        "\n",
        "# 5-fold CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "for p in params:\n",
        "  boost_mod = XGBClassifier(n_estimators=int(p[0]), learning_rate=p[1], max_depth=int(p[2]), colsample_bytree=.1, subsample=.1, tree_method=\"gpu_hist\")   # create boosting model\n",
        "  error = []\n",
        "  for train_index, test_index in skf.split(data, labels):\n",
        "    X_train, X_test = data.iloc[train_index].to_numpy(), data.iloc[test_index].to_numpy()  # train-test split\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    boost_mod.fit(X_train, y_train)    # fit on train\n",
        "    y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "    error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(sum(error)/len(error), p)  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8680357142857144 [3.e+02 1.e-02 4.e+00]\n",
            "0.9125 [5.e+02 5.e-02 2.e+00]\n",
            "0.9333928571428572 [7.e+02 5.e-02 3.e+00]\n",
            "0.9141071428571429 [7.e+02 3.e-02 3.e+00]\n",
            "0.9308928571428572 [7.e+02 7.e-02 4.e+00]\n",
            "0.6789285714285714 [1.e+02 1.e-02 4.e+00]\n",
            "0.8955357142857142 [5.e+02 3.e-02 4.e+00]\n",
            "0.8532142857142857 [2.e+02 7.e-02 4.e+00]\n",
            "0.8144642857142858 [2.e+02 1.e-02 4.e+00]\n",
            "0.7126785714285714 [1.e+02 5.e-02 2.e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaDTS9z9G_pX",
        "colab_type": "text"
      },
      "source": [
        "My initial trial is not too promising with a best score of 93%, but I will try again without stochastic boosting (100% of rows and cols used). In order to speed things up, I will use only the top 500 features selected previously. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TwWaStJOkJv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title XGBoost, RandomSearch, top 500 features\n",
        "#Boosting, Random grid search, XGboost, feature selection\n",
        "\n",
        "random.seed(711)\n",
        "# create a default XGBoost classifier\n",
        "boost_mod = XGBClassifier(tree_method=\"gpu_hist\") \n",
        "# create K-fold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "# Create the grid search parameter grid and scoring funcitons\n",
        "param_grid = {\n",
        "    \"learning_rate\": [0.1, .07, .05, .03, 0.01],\n",
        "    \"max_depth\": [2, 3, 4],\n",
        "    \"n_estimators\": [100, 200, 300, 500, 700],\n",
        "}\n",
        "# create the grid search object\n",
        "grid = RandomizedSearchCV(estimator=boost_mod, param_distributions=param_grid,cv=skf,scoring='accuracy',n_jobs=-1,n_iter=10)\n",
        "# fit grid search\n",
        "best_model = grid.fit(data_best500.to_numpy(), labels)\n",
        "\n",
        "print(f'Best score: {best_model.best_score_}')\n",
        "print(f'Best model: {best_model.best_params_}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1S3pE2vbI35",
        "colab_type": "code",
        "outputId": "93cfab06-44fe-4ed7-cc3d-ed1621a7998e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "pd.DataFrame(best_model.cv_results_).iloc[:,[4,5,6,-3]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>param_n_estimators</th>\n",
              "      <th>param_max_depth</th>\n",
              "      <th>param_learning_rate</th>\n",
              "      <th>mean_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>500</td>\n",
              "      <td>3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.936782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>700</td>\n",
              "      <td>4</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.931034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>700</td>\n",
              "      <td>3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.936782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.942529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>300</td>\n",
              "      <td>4</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.936782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>200</td>\n",
              "      <td>3</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.931034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>200</td>\n",
              "      <td>4</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.885057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>300</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.936782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>700</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.936782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>700</td>\n",
              "      <td>2</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.942529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  param_n_estimators param_max_depth param_learning_rate  mean_test_score\n",
              "0                500               3                0.05         0.936782\n",
              "1                700               4                0.07         0.931034\n",
              "2                700               3                0.05         0.936782\n",
              "3                300               2                0.03         0.942529\n",
              "4                300               4                0.05         0.936782\n",
              "5                200               3                0.07         0.931034\n",
              "6                200               4                0.01         0.885057\n",
              "7                300               3                 0.1         0.936782\n",
              "8                700               4                 0.1         0.936782\n",
              "9                700               2                0.07         0.942529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fMgJDyQIJbv",
        "colab_type": "text"
      },
      "source": [
        "Results are slightly better, with a top score of 94%. I will repeat the above search with the top 1000 and 2000 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K26q8guUmOlv",
        "colab_type": "code",
        "outputId": "4adbba9c-e3e5-4670-d5d0-86c146745ada",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#@title XGBoost, RandomSearch, top 1000 features\n",
        "#Boosting, Random grid search, XGboost, best 1000 features\n",
        "\n",
        "random.seed(711)\n",
        "# create a default XGBoost classifier\n",
        "boost_mod = XGBClassifier(tree_method=\"gpu_hist\") \n",
        "\n",
        "# create K-fold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "# Create the grid search parameter grid and scoring funcitons\n",
        "param_grid = {\n",
        "    \"learning_rate\": [0.1, .07, .05, .03, 0.01],\n",
        "    \"max_depth\": [2, 3, 4],\n",
        "    \"n_estimators\": [100, 200, 300, 500, 700],\n",
        "}\n",
        "scoring = 'accuracy'\n",
        "\n",
        "# create the grid search object\n",
        "grid = RandomizedSearchCV(\n",
        "    estimator=boost_mod, \n",
        "    param_distributions=param_grid,\n",
        "    cv=skf,\n",
        "    scoring=scoring,\n",
        "    n_jobs=-1,\n",
        "    n_iter=10,\n",
        ")\n",
        "# fit grid search\n",
        "best_model = grid.fit(data_best1000.to_numpy(), labels)\n",
        "\n",
        "print(f'Best score: {best_model.best_score_}')\n",
        "print(f'Best model: {best_model.best_params_}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.9712643678160919\n",
            "Best model: {'n_estimators': 700, 'max_depth': 4, 'learning_rate': 0.05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzUrVZmGsPOC",
        "colab_type": "code",
        "outputId": "85383f81-e8d8-498e-eb99-ddd3201e8776",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#@title XGBoost, RandomSearch, top 2000 features\n",
        "#Boosting, Random grid search, XGboost, best 2000 feature selection\n",
        "\n",
        "random.seed(711)\n",
        "# create a default XGBoost classifier\n",
        "boost_mod = XGBClassifier(tree_method=\"gpu_hist\") \n",
        "# create K-fold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "# Create the grid search parameter grid and scoring funcitons\n",
        "param_grid = {\n",
        "    \"learning_rate\": [0.1, .07, .05, .03, 0.01],\n",
        "    \"max_depth\": [2, 3, 4],\n",
        "    \"n_estimators\": [100, 200, 300, 500, 700],\n",
        "}\n",
        "scoring = 'accuracy'\n",
        "\n",
        "# create the grid search object\n",
        "grid = RandomizedSearchCV(\n",
        "    estimator=boost_mod,param_distributions=param_grid,cv=skf,scoring=scoring,n_jobs=-1,n_iter=10)\n",
        "# fit grid search\n",
        "best_model = grid.fit(data_best2000.to_numpy(), labels)\n",
        "\n",
        "print(f'Best score: {best_model.best_score_}')\n",
        "print(f'Best model: {best_model.best_params_}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.9827586206896551\n",
            "Best model: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.03}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT1QlrB4Iw_P",
        "colab_type": "text"
      },
      "source": [
        "This is much better. With 1000 variables, 97.1% was acheived, and with 2000, 98.2%. I will using these parameters with all the features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQtI4GLFq2Is",
        "colab_type": "code",
        "outputId": "73a5f4f1-3a47-449f-a9b0-43bec35f3c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# XGBoost, 5-fold CV, all features used\n",
        "random.seed(1699)\n",
        "boost_mod = XGBClassifier(n_estimators=700,learning_rate=.05,max_depth=4, tree_method=\"gpu_hist\")   # create boosting model\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "error = []\n",
        "for train_index, test_index in skf.split(data, labels):\n",
        "  X_train, X_test = data.iloc[train_index].to_numpy(), data.iloc[test_index].to_numpy()  # train-test split\n",
        "  y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "  boost_mod.fit(X_train, y_train)    # fit on train\n",
        "  y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "  error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "print(sum(error)/len(error))  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9198214285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdyXL-_tJ2EG",
        "colab_type": "text"
      },
      "source": [
        "Adding in the rest of the features did not help, and in fact detracted quite a bit, likely due to higher variance. I notice that higher numbers of trees seem to perform well (700 trees, at the upper end of the search). I now a search with higher numbers of trees, between 600 and 1200. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2pTTnT0JLSh",
        "colab_type": "code",
        "outputId": "4a2e68e3-d01e-40be-9e08-12095b86958d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Another grid search, more trees.\n",
        "#XGBoost, RandomSearch, top 2000 features\n",
        "\n",
        "random.seed(711)\n",
        "# create a default XGBoost classifier\n",
        "boost_mod = XGBClassifier(tree_method=\"gpu_hist\") \n",
        "# create K-fold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "# Create the grid search parameter grid and scoring funcitons\n",
        "param_grid = {\n",
        "    \"learning_rate\": stats.uniform(loc=.01,scale=.99),\n",
        "    \"max_depth\": [2, 4],\n",
        "    \"n_estimators\": np.arange(600,1200),\n",
        "}\n",
        "\n",
        "# create the grid search object\n",
        "grid = RandomizedSearchCV(estimator=boost_mod, param_distributions=param_grid, cv=skf, scoring='accuracy', n_jobs=-1, n_iter=10)\n",
        "# fit grid search\n",
        "best_model = grid.fit(data_best2000.to_numpy(), labels)\n",
        "\n",
        "print(f'Best score: {best_model.best_score_}')\n",
        "print(f'Best model: {best_model.best_params_}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.9827586206896551\n",
            "Best model: {'learning_rate': 0.5255287278569792, 'max_depth': 2, 'n_estimators': 970}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "419xxTFULMIL",
        "colab_type": "text"
      },
      "source": [
        "Again, good, but no improvement over the previous. We still have not beaten random forest at 98.8%\n",
        "\n",
        "Now comes a twist in my tale: by chance I ran a model with the following parameters, using some mild stochastic boosting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzpTVnJAA10S",
        "colab_type": "code",
        "outputId": "63399ba7-ef5a-46b4-de36-9805bbb54b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# XGBoost, 5-fold CV, all features\n",
        "random.seed(1699)\n",
        "boost_mod = XGBClassifier(n_estimators=100,learning_rate=.1,max_depth=2, tree_method=\"gpu_hist\", colsample_bytree=.7, subsample=.7)   # create boosting model\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "error = []\n",
        "for train_index, test_index in skf.split(data, labels):\n",
        "  X_train, X_test = data.iloc[train_index].to_numpy(), data.iloc[test_index].to_numpy()  # train-test split\n",
        "  y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "  boost_mod.fit(X_train, y_train)    # fit on train\n",
        "  y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "  error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "print(sum(error)/len(error))  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tu-zvH3MElF",
        "colab_type": "text"
      },
      "source": [
        "This is the highest score yet! I now do a RandomSearch around those parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2VbYoeUr75L",
        "colab_type": "code",
        "outputId": "e08350a4-e666-4dff-c516-e6c2bd1b7e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# random search around the lucky numbers\n",
        "# XGBoost, all features\n",
        "random.seed(711)\n",
        "n_iter = 10 # n random searches\n",
        "\n",
        "# search parameters\n",
        "n_trees = np.arange(70,130) \n",
        "rate = np.linspace(.05,.5,50)\n",
        "cols_samp = np.linspace(.5,1,50)\n",
        "subsamp = np.linspace(.3,1,50)\n",
        "params = np.array([np.random.choice(n_trees,n_iter),np.random.choice(rate,n_iter),np.random.choice(cols_samp,n_iter),np.random.choice(subsamp,n_iter)]).T\n",
        "\n",
        "# 5-fold CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "for p in params:\n",
        "  boost_mod = XGBClassifier(n_estimators=int(p[0]), learning_rate=p[1], max_depth=2, colsample_bytree=p[2], subsample=p[3], tree_method=\"gpu_hist\")   # create boosting model\n",
        "  error = []\n",
        "  for train_index, test_index in skf.split(data, labels):\n",
        "    X_train, X_test = data.iloc[train_index].to_numpy(), data.iloc[test_index].to_numpy()  # train-test split\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    boost_mod.fit(X_train, y_train)    # fit on train\n",
        "    y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "    error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "  print(\"Accuracy:\",sum(error)/len(error),\"params:\", p)  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9875 params: [105.       0.2337   0.7755   0.6143]\n",
            "Accuracy: 0.9830357142857142 params: [126.       0.4449   0.5408   0.7429]\n",
            "Accuracy: 0.9698214285714286 params: [87.      0.3622  0.7551  0.4429]\n",
            "Accuracy: 0.9824999999999999 params: [9.0000e+01 6.8367e-02 6.8367e-01 6.7143e-01]\n",
            "Accuracy: 0.9603571428571428 params: [87.      0.1235  0.949   0.8714]\n",
            "Accuracy: 0.9583928571428573 params: [125.       0.5      0.5      0.5714]\n",
            "Accuracy: 0.9823214285714286 params: [109.       0.3806   0.8061   0.4714]\n",
            "Accuracy: 0.9885714285714287 params: [98.      0.1418  0.8673  0.7714]\n",
            "Accuracy: 0.9817857142857143 params: [1.2000e+02 1.0510e-01 9.3878e-01 9.0000e-01]\n",
            "Accuracy: 1.0 params: [74.      0.2429  0.6939  0.5857]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tVoN58jMNVg",
        "colab_type": "text"
      },
      "source": [
        "100% accuracy with 5-fold CV! To veryfy I repeat with those parameters and a different seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35g9PFkL-adU",
        "colab_type": "code",
        "outputId": "0aae3176-048a-4f32-ef7a-e154ea6813f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Boosting, 5-fold CV, XGBoost\n",
        "random.seed(7211)\n",
        "boost_mod = XGBClassifier(n_estimators=74,learning_rate=.2429,max_depth=2, tree_method=\"gpu_hist\", colsample_bytree=.6939, subsample=.5857)   # create boosting model\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5-fold\n",
        "error = []\n",
        "for train_index, test_index in skf.split(data, labels):\n",
        "  X_train, X_test = data.iloc[train_index].to_numpy(), data.iloc[test_index].to_numpy()  # train-test split\n",
        "  y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "  boost_mod.fit(X_train, y_train)    # fit on train\n",
        "  y_pred = boost_mod.predict(X_test)  # predict on test\n",
        "  error.append(metrics.accuracy_score(y_test, y_pred))   # model accuracy\n",
        "print(sum(error)/len(error))  # average accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9942857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGt92d7HMu3m",
        "colab_type": "text"
      },
      "source": [
        "Incredible! Boosting (with a lot of tuning) was able to beat random forest by a small margin.\n",
        "\n",
        "Below is a simple summary of the best reult for each type of model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv3v6J9gAZz0",
        "colab_type": "code",
        "outputId": "1eb91fca-5aa4-445c-b15f-e630d107ac9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Results Summary\n",
        "print(\"Model:\\t\\t\\t\", \"Accuracy:\")\n",
        "print(\"Naive Bayes\\t\\t\\t96.8%\")\n",
        "print(\"Random Forest\\t\\t\\t98.8%\")\n",
        "print(\"Boosting\\t\\t\\t92.3%\")\n",
        "print(\"Boosting, var selection\\t\\t98.4%\")\n",
        "print(\"Boosting, stochastic\\t\\t99.4%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model:\t\t\t Accuracy:\n",
            "Naive Bayes\t\t\t96.8%\n",
            "Random Forest\t\t\t98.8%\n",
            "Boosting\t\t\t92.3%\n",
            "Boosting, var selection\t\t98.4%\n",
            "Boosting, stochastic\t\t99.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geokk2wdqY4-",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "From a biological standpoint, the most useful part of this study is the variable importance: finding out which genes are most important in which types of cancer. Even then, it would have been more useful to compare cancerous cells of one type to non-cancerous cells of the same type. As it stands, it is unclear whether important genes are unique to the cancer type, or simply to the cell type (eg. genes unique to the brain vs genes unique to the liver). But that was not the goal of this model: oncogene identification is another thing entirely. \n",
        "\n",
        "From a data science standpoint, These models did unexpectedly well for relatively low sample size and high dimensionality, with a large number of classes. Random Forest performed incredibly well with almost no tuning. Boosting took and incredible ammount of tuning to outperform RF, but eventually did beat RF by a bit. Interestigly, feature selection ended up not being used in the final model. \n",
        "\n"
      ]
    }
  ]
}